\documentclass[]{article}

\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{graphicx}
\usepackage{amsmath,mathtools,amssymb}
\newcommand{\code}[1]{\texttt{#1}}
\usepackage{subfigure}

\usepackage{geometry}
 \geometry{
letterpaper,
 total={215mm,279mm},
 left=45mm,
 right=45mm,
 top=25mm,
tmargin = 15mm,
 bottom=25mm,
 }


\begin{document}

\title{Homework 1}
\author{Joshua Michalenko\\ ELEC 677: Deep Learning \\  Dr.\,Ankit Patel}
\date{10/04/16}
\maketitle



\section{Problem 1: Backpropogation in a simple Nueral Network}
\subsection{Part A:  Dataset}
The plot of the Two Moons dataset is displayed in Figure \ref{fig:partA}

\begin{figure}[ht]
        \centering
        \includegraphics[width=10cm]{figures/twoMoons.png}

 	\caption{Two moons dataset displayed in matplotlib}

 	 \label{fig:partA}
\end{figure}


\subsection{Part B - Activation Functions}
\subsubsection{Part B1 - Implement activation functions}
If you look in my code you can see I implemented the activations function. 


\subsubsection{Part B2 - Derivative Derivations}
\paragraph{ Part B2.1 -  Derivative for Sigmoid function}

\begin{align*} 
\sigma(z)= f(z) = \frac{1}{1+e^{-z}}  &= (1+ e^{-z})^{-1} \\
\frac{df}{dz} : = -1* (1+ e^{-z})^{-2} * -e^{-z}  &= e^{-z}(1+ e^{-z})^{-2}  \text{~~~(by chain rule)}\\
 &= \frac{e^{-z}}{(1+ e^{-z})^{2}  } \\
 &= \frac{1-e^{-z}+1}{(1+ e^{-z})^{2}  }\text{  ~~~(add and subtract a 1)} \\
 &= \frac{1+e^{-z}}{(1+ e^{-z})^{2} } -  \frac{1}{(1+ e^{-z})^{2} }  \text{  ~~~(split terms)}\\
 &= \frac{1}{1+ e^{-z} } -  \frac{1}{(1+ e^{-z})^{2} }  \\
 &= \sigma(z) -  \frac{1}{(1+ e^{-z})^{2} }  \\
 &= \sigma(z) -  \left ( \frac{1}{(1+ e^{-z}) } \right )^2  \\
 &= \sigma(z) -  \sigma(z)^2  \\
\Aboxed{f'(z) &= \sigma(z) (1-  \sigma(z)) } \\
\end{align*}

\paragraph{Part B2.2 - Derivative for Tanh function}

\begin{align*} 
f(z) = \text{tanh}(z) &= \frac{\text{sinh}(z)}{\text{cosh}(z)} \\
&= \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
\frac{df}{dz} : &=  \frac{\text{cosh}(z) * \text{sinh}'(z) -  \text{sinh}(z)  * \text{cosh}'(z)}{(\text{cosh}(z))^2}  \text{~~~(by quotient rule)} \\ 
&=  \frac{\text{cosh}(z)^2 -  \text{sinh}(z)^2 }{(\text{cosh}(z))^2} \\ 
&=  \frac{\text{cosh}(z)^2}{\text{cosh}(z)^2} - \frac{\text{sinh}(z)^2}{\text{cosh}(z)^2}\\ 
\Aboxed{f'(z) &= 1- \text{tanh}^2(z)}\\ 
\end{align*}


\paragraph{Part B2.3 - Derivative for ReLu function}
\begin{align*} 
\text{ReLu}(z) = f(z)& = \text{max}(0,z)\\
\Aboxed{ \frac{df}{dz} : &= \left\{
    \begin{array}{ll}
      0 ~~~ z\leq 0 \\ 
      1 ~~~ z\geq 0 \\
    \end{array}
  \right. } \text{(I think the dirivitive here is pretty obvious)}\\
\end{align*}


\subsubsection{Part B3 - Implement activation function gradiants}
If you look in my code you can see I implemented the activations function gradiants. 

\subsection{Part C - Build the Neural Network}
If you look in my code you can see I implemented the feedforward and loss functions
\subsection{Part D - Backpropogation Derivations}
\subsubsection{Derivations of Backpropogation Equations}
Keep in mind that the process for a single feedforward pass of the network is defined by the following equations with $x\in \Re^{p}$ being a single data sample with $p$ features, $\mathbf{W_1} \in \Re^{n_1  \times p}$ is a weight matrix transitioning from the input later with $p$ features to the number of units in hidden layer 1 $n_1$, $b_1\in \Re^{n_1}$ is a bias vector, $z_1\in \Re^{n_1}$ are a vector of potentials for layer 1,  $a_1\in \Re^{n_1}$ the corresponding activations, and $\hat{y}$ are the resulting probabilities. $\mathbf{W_2} \in \Re^{n_2 \times n_1}$ , $a_1 \in \Re^{n_1}$, $z_2 \in \Re^{n_2}$

\begin{align*}
z_1 &= \mathbf{W_1}x + b_1\\
a_1 &= \text{actFun}(z_1)\\
z_2 &= \mathbf{W_2}a_1+b_2 \\
a_2 = \hat{y} &= \text{softmax}(z_2) \\ 
\end{align*}

Where the softmax function is given by:
\begin{align*}
\text{softmax}(\mathbf{z})_c = \frac{e^{z_c}}{\sum_{d=1}^C e^{z_d}} = \frac{e^{z_c}}{ \Sigma_C }\quad \text{for} \; c = 1 \cdots C
\end{align*}

With a cross entropy loss function of :
\begin{align*}
L(y,\hat{y}) = -  \frac{1}{N} \sum_{n \in N} \sum_{j\in C} y_{n,j} \text{log}(\hat{y}_{n,j} ) \\
\end{align*}

With $y$ being a one hot vector of the correct label, $N$ being the number of training examples and $C$ being the number of classes. \\

For the following derivations, the derivative of the softmax $\frac{\partial \hat{y}_i}{\partial z_j}$ is useful to know. It is calculated below. It is the derivative of $\hat{y}$ with respect to one of the elements of the input vector $z$.

\begin{align*}
\hat{y} = \text{softmax}(\mathbf{z})_c = \frac{e^{z_c}}{\sum_{d=1}^C e^{z_d}} &= \frac{e^{z_c}}{ \Sigma_C }\quad \text{for} \; c = 1 \cdots C \\
\text{if} \; i = j :~~ \frac{\partial y_i}{\partial z_i} &= \frac{\partial \frac{e^{z_i}}{\Sigma_C}}{\partial z_i} \\
&= \frac{e^{z_i}\Sigma_C - e^{z_i}e^{z_i}}{\Sigma_C^2} \\ 
&= \frac{e^{z_i}}{\Sigma_C}\frac{\Sigma_C - e^{z_i}}{\Sigma_C}  \\
&= \frac{e^{z_i}}{\Sigma_C}(1-\frac{e^{z_i}}{\Sigma_C}) \\
&=  y_i (1 - y_i) \\
\text{if} \; i \neq j :~~ \frac{\partial y_i}{\partial z_j} &= \frac{\partial \frac{e^{z_i}}{\Sigma_C}}{\partial z_j} \\ 
&= \frac{0 - e^{z_i}e^{z_j}}{\Sigma_C^2} \\
&= -\frac{e^{z_i}}{\Sigma_C} \frac{e^{z_j}}{\Sigma_C} \\
&= -y_i y_j \\
\end{align*}


The derivative of the cross entropy loss with respect to the second layer inputs are also a useful quantity to calculate, it is derived as follows. Note that I used the cross entropy for one example. Taking out the summation here makes things easier but I'll add it back in later.

\begin{align*}
 \frac{\partial L }{\partial  \hat{y}_{j} }  \frac{\partial  \hat{y}_{j} }{\partial z_{2_i}}  =\frac{\partial L}{\partial z_{2_i}} &= - \sum_{j=1}^C \frac{\partial y_j log(\hat{y}_j)}{\partial z_i}{} \\
&=- \sum_{j=1}^C y_j \frac{\partial log(\hat{y}_j)}{\partial z_i} \\
&= - \sum_{j=1}^C y_j \frac{1}{\hat{y}_j} \frac{\partial \hat{y}_j}{\partial z_i} \\
&= - \frac{y_i}{\hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} - \sum_{j \neq i}^C \frac{y_j}{\hat{y}_j} \frac{\partial \hat{y}_j}{\partial z_i} \quad \text{substitution from above derivation}\\
&= - \frac{y_i}{\hat{y}_i} \hat{y}_i (1-\hat{y}_i) - \sum_{j \neq i}^C \frac{y_j}{\hat{y}_j} (-\hat{y}_j \hat{y}_i) \\
&= - y_i + y_i \hat{y}_i + \sum_{j \neq i}^C y_j \hat{y}_i  \\
&= - y_i + \sum_{j = 1}^C y_j \hat{y}_i \\
&= -y_i + \hat{y}_i \sum_{j = 1}^C y_j \\
& = \hat{y}_i - y_i \\
\end{align*}
\paragraph{ $\frac{dL}{dW_2}$ Derivation}
We can rewrite the loss function with substituting in the feedforward equations above as follows.

\begin{align*}
L(y,\hat{y}) &= -  \frac{1}{N} \sum_{n \in N} \sum_{j\in C} y_{n,j} \text{log}(\hat{y}_{n,j} ) \\
\frac{\partial L}{\partial W_2} : &=  \frac{\partial L }{\partial  \hat{y}_{n,j} }      \frac{\partial  \hat{y}_{n,j} }{\partial z_2} \frac{\partial z_2}{\partial W_2}\\
\text{Since j-th element of $z_2$ is given by: } \\
z_{2_j} &= \sum_{k = 0}^{n_1}w_{j,k}a_{1_k} + b_{2_j}\\ 
\text{The derivitive of the $j$-th element }\\
\text{of $z_2$ w.r.t. weight $w_{j,k}$ is then: }\\
\frac{\partial z_{2_j}}{\partial w_{j,k}} &= a_{1_k}\\
\text{Using the derivation above for  $\frac{\partial L}{\partial z_{2_j}}$}\\
\frac{\partial L}{\partial w_{2_{j,k}}} &=  (\hat{y}_j - y_j )a_{1_k} \\
\therefore \\
\frac{dL}{dW_2} &= (\mathbf{\hat{y} - y})\mathbf{a_1^T} \in  \Re^{n_2 \times n_1} \\\text{The above equation only takes} & \text{ into consideration on data point} \\
\text{We extend this to multiple samples by} & \text{ plugging in the summation we originally took out.}\\
\Aboxed{ \frac{dL}{dW_2} &= \frac{1}{N} \sum_{n \in N} (\mathbf{\hat{y}_n - y_n})\mathbf{a_1^T} \in  \Re^{n_2 \times n_1}}\\
\end{align*}

\paragraph{$\frac{dL}{db_2}$ Derivation}
This derivation will be fairly similar to the previous section except now the gradient looks as follows:

\begin{align*}
\frac{\partial L}{\partial b_2} : &=  \frac{\partial L }{\partial  \hat{y}_{n,j} } \frac{\partial  \hat{y}_{n,j} }{\partial z_2} \frac{\partial z_2}{\partial b_2}\\
\text{Since j-th element of $z_2$ is given by: } \\
z_{2_j} &= \sum_{k = 0}^{n_1}w_{j,k}a_{1_k} + b_{2_j}\\ 
\text{The derivitive of the $j$-th element }\\
\text{of $z_2$ w.r.t. bias $b_{2_k}$ is then: }\\
\frac{\partial z_{2_j}}{\partial b_{2_k}} &= 1\\
\text{Using the derivation above for  $\frac{\partial L}{\partial z_{2_j}}$}\\
\frac{\partial L}{\partial b_{2_k}} &=  (\hat{y}_j - y_j )\\
\therefore \\
\frac{dL}{db_2} &= \mathbf{\hat{y} - y}\in  \Re^{n_2} \\
\text{The above equation only takes} & \text{ into consideration on data point.} \\
\text{We extend this to multiple samples by} & \text{ plugging in the summation we originally took out.}\\
\Aboxed{ \frac{dL}{db_2} &= \frac{1}{N} \sum_{n \in N}  \mathbf{\hat{y}_n - y_n}\in  \Re^{n_2}}\\
\end{align*}

\paragraph{$\frac{dL}{dW_1}$ Derivation}
We add one more layer to our previous derivation of $\frac{dL}{dW_2}$ for this derivation. The derivative can be expanded as follows

\begin{align*}
\frac{\partial L}{\partial W_1} : &=  \frac{\partial L }{\partial  \hat{y}_{n,j} } \frac{\partial  \hat{y}_{n,j} }{\partial z_2} \frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1} \frac{\partial z_1}{\partial W_1}  \\
\end{align*}

So we need to figure out the term $\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1} \frac{\partial z_1}{\partial W_1} $by piecing to out. The $j$-th element of $z_1$ is given by:

\begin{align*}
z_{1_j} &= \sum_{i = 0}^p w_{1_{j,i}}x_i + b_{1_j} \\
\text{So the derivative w.r.t. $w_{1_{j,i}}$ is then:}\\
\frac{\partial z_{1_j} }{\partial w_{1_{j,i}}} &= x_i \\
\end{align*}

We know from part b that $\frac{\partial a_1}{\partial z_1} $ is dependent on the type of activation function but we derived all the derivatives of the three activation functions above so stating the derivatives is redundant. See the above part B. 

\begin{align*}
\frac{\partial a_1}{\partial z_1} & =  \text{actFun}'(z_1) \quad \text{ (See part B above)}
\end{align*}


The last part we need to derive is the $\frac{\partial z_2}{\partial a_1}$ term. 

\begin{align*}
\text{Since the j-th element } & \text{of $z_2$ is given by: } \\
z_{2_j} &= \sum_{k = 0}^{n_1}w_{2_{j,k}}a_{1_k} + b_{2_j}\\ 
\text{we can conclude that the}&\text{ derivative w.r.t. $a_{1_k}$ is given by }\\ 
\frac{\partial z_{2_j}}{\partial a_{1_k}} & =w_{2_{j,k}}\\
\text{From above, we know that}&\text{ the other partial derivatives are}\\
\frac{\partial L }{\partial  \hat{y}_{j} }  \frac{\partial  \hat{y}_{j} }{\partial z_{2_i}}  &=\frac{\partial L}{\partial z_{2_i}} = \hat{y}_i - y_i \\
\text{So if we piece this all } & \text{together we come up with:}\\
\\
\frac{\partial L}{\partial w_{1_{k,p}}} : &=  \frac{\partial L }{\partial  \hat{y}_{n,c} } \frac{\partial  \hat{y}_{n,c} }{\partial z_{2_c}} \frac{\partial z_{2_c}}{\partial a_{1_k}}\frac{\partial a_{1_k}}{\partial z_{1_k}} \frac{\partial z_{1_k}}{\partial w_{1_{k,p}}}  
\end{align*}

Which I changed some of the indexing to make the most sense. In this format $n \in N$, $c \in C$,   , $k \in \{0,... , n_1-1  \}$, $p \in \{0,... , P-1  \}$ with $P$ input features. The resulting derivative with respect to one weight in the first layer $w_{1_{k,p}}$ is then:

\begin{align*}
\frac{\partial L}{\partial w_{1_{k,p}}} &= \sum_{c\in C}( \hat{y}_c - y_c) w_{2_{c,k}}\text{actFun}'(z_{1_k})x_p \\
\text{and the extention}&\text{ to matrix form is then} \\
\frac{\partial L}{\partial \mathbf{W_1}} &= (\mathbf{ \hat{y}} - \mathbf{y}) \mathbf{W_2}  \odot \text{actFun}'(\mathbf{z_1})\mathbf{x^T} \in \Re^{n_1 \times p} \\
\text{If there is more}&\text{than one sample (which there always is)}\\
\text{then we average}&\text{over all of the samples and the final solution is then} \\
\Aboxed{\frac{\partial L}{\partial \mathbf{W_1}} &=  \frac{1}{N} \sum_{n\in N} (\mathbf{ \hat{y}} - \mathbf{y}) \mathbf{W_2}  \odot \text{actFun}'(\mathbf{z_1})\mathbf{x^T} \in \Re^{n_1 \times p}} \\
\end{align*}

\paragraph{$\frac{dL}{db_1}$ Derivation}
The derivation is very similar to the $\frac{dL}{dW_1}$ except the last part of the chain of partial derivatives is changed. The gradient equation of the Loss function with respect to a single bias term in the first layer is 

\begin{align*}
\frac{\partial L}{\partial b_{1_{k}}} : &=  \frac{\partial L }{\partial  \hat{y}_{n,c} } \frac{\partial  \hat{y}_{n,c} }{\partial z_{2_c}} \frac{\partial z_{2_c}}{\partial a_{1_k}}\frac{\partial a_{1_k}}{\partial z_{1_k}} \frac{\partial z_{1_k}}{\partial b_{1_{k}}}  \\
\end{align*}

So we only need to calculate $\frac{\partial z_{1_k}} {\partial b_{1_{k}} }$ first. Let's look at how the $k$ th value is calculated. 

\begin{align*}
z_{1_k} &= \sum_{i = 0}^p w_{1_{k,i}}x_i + b_{1_k} \\
\text{So the derivative w.r.t. $b_{1_{k}}$ is then:}\\
\frac{\partial z_{1_j} }{\partial b_{1_{k}}} &= 1 \\
\end{align*}

The partial derivative  $\frac{\partial z_{1_k}} {\partial b_{1_{k}} } =1$ simplified the equation for the partial derivative of the loss function w.r.t. $b_{1_{k}} $ as

\begin{align*}
\frac{\partial L}{\partial b_{1_{k}}} &= \sum_{c\in C}( \hat{y}_c - y_c) w_{2_{c,k}}\text{actFun}'(z_{1_k})\\
\text{and the resulting matrix} &\text{ form is then}\\
\frac{\partial L}{\partial \mathbf{b_1}} &=(\mathbf{ \hat{y}} - \mathbf{y}) \mathbf{W_2^T} \text{actFun}'(\mathbf{z_1})\in \Re^{n_1} \\
\text{If there is more than one}&\text{ sample (which there always is) then we}\\
\text{have to sum up and}& \text{ average over all samples and the gradient becomes}\\
\Aboxed{ \frac{\partial L}{\partial \mathbf{b_1}} &=  \frac{1}{N} \sum_{n\in N}( \mathbf{ \hat{y}} - \mathbf{y}) \mathbf{W_2^T} \odot \text{actFun}'(\mathbf{z_1})\in \Re^{n_1} } \\
\end{align*}

\subsubsection{Implementation of Derivations of Backpropogation Equations}
If you look in my code you can see I implemented the backpropogation equations correctly.

\subsection{Time to Have Fun - Training!}
\subsubsection{Change the activation functions and number of hidden units Part 1 and 2}
The decision boundaries are shown for tanh, sigmoid, and relu functions for 3, 10 and 20 hidden hidden units.
Using the default 3 hidden units with different activation functions, we get Figures \ref{fig:3hid1},\ref{fig:3hid2}, and \ref{fig:3hid3}.
\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_3hid_tanh_pt260loss.png}
    \end{subfigure}%
    \caption{Default settings, 3 hidden units with tanh activation fuctions. Loss = .260}
\label{fig:3hid1}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_3hid_sigmoid_pt304loss.png}
    \end{subfigure}
    \caption{3 hidden units with sigmoid activation fuctions. Loss = .304}
\label{fig:3hid2}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_3hid_relu_pt305loss.png}
    \end{subfigure}
    \caption{3 hidden units with ReLU activation fuctions. Loss = .305}
\label{fig:3hid3}
\end{figure*}

Increasing the number of hidden units to 10 gives us figures \ref{fig:10hid1}, \ref{fig:10hid2} and  \ref{fig:10hid3}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_10hid_tanh_pt246loss.png}
    \end{subfigure}%
    \caption{10 hidden units with tanh activation fuctions. Loss = .246}
 \label{fig:10hid1}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_10hid_sigmoid_pt301loss.png}
    \end{subfigure}
    \caption{10 hidden units with sigmoid activation fuctions. Loss = .301}
 \label{fig:10hid2}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_10hid_relu_pt302loss.png}
    \end{subfigure}
    \caption{10 hidden units with ReLU activation fuctions. Loss = .302}
 \label{fig:10hid3}
\end{figure*}

Increasing the number of hidden units to 20 gives us figure \ref{fig:20hid1}, \ref{fig:20hid2} and  \ref{fig:20hid3}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_20hid_tanh_pt191loss.png}
    \end{subfigure}%
    \caption{20 hidden units with tanh activation fuctions. Loss = .191}
 \label{fig:20hid1}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_20hid_sigmoid_pt288loss.png}
    \end{subfigure}
    \caption{20 hidden units with sigmoid activation fuctions. Loss = .288}
 \label{fig:20hid2}
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[height=2.5in]{figures/default_20hid_relu_pt300loss.png}
    \end{subfigure}
    \caption{20 hidden units with ReLU activation fuctions. Loss = .300}
 \label{fig:20hid3}
\end{figure*}

The Tanh activation function seems to be the most expressive, it curves much more than the sigmoid and relu activations functions. The Relu activation function decision boundary looks piecewise linear which makes sense because it is. The sigmoid function looks practically linear for a small number of hidden units, but as the number of hidden units is increased, we seem more expressively in the decision boundary. 

As we increase the number of hidden units in the Tanh network specifically, the decision boundary becomes very expressive and using 20 hidden units, is almost able to create a decision boundary that separates the two classes. 

\subsection{Building a deeper network}

\end{document}





















